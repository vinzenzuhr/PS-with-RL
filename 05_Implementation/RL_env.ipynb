{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54418ad-b423-494c-907e-00dfa00a6011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /opt/conda/lib/python3.11/site-packages (0.29.1)\n",
      "Requirement already satisfied: torch_geometric in /opt/conda/lib/python3.11/site-packages (2.5.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (4.66.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (1.11.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (2023.9.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (3.10.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (3.1.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (2024.7.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->torch_geometric) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600e7dfa-0283-46dc-92fb-f3f4c36fff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e692ee49-1146-40b6-a2cd-e297e8cace24",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7082fcad-8b48-4910-95c6-d398dd649e90",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9873b04c-9b3d-48f6-845c-1bede3b252ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL env bauen\n",
    "from collections import Counter\n",
    "import copy\n",
    "import itertools\n",
    "from typing import Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "class PersonnelScheduleEnv(gym.Env):\n",
    "    def __init__(self, employees: torch.tensor, shifts: torch.tensor, assignments: torch.tensor, device) -> None:        \n",
    "        # setup graph\n",
    "        self.device = device\n",
    "        self.initial_state=HeteroData()\n",
    "        self.initial_state[\"employee\"].x = employees\n",
    "        self.initial_state[\"shift\"].x = shifts\n",
    "        self.initial_state[\"employee\", \"assigned\", \"shift\"].edge_index = assignments\n",
    "        self.initial_state = self.initial_state.to(self.device)\n",
    "        if self.initial_state.validate(raise_on_error=True):\n",
    "            print(\"Data validation successful\") \n",
    "        else: \n",
    "            raise ValueError(\"Data validation not successful\")\n",
    "        print(\"The graph has \", self.initial_state.num_nodes, \" nodes\")\n",
    "        print(\"The graph has \", self.initial_state.num_edges, \" edges\")\n",
    "\n",
    "        self.edge_space = list(itertools.product(range(employees.shape[0]),range(shifts.shape[0])))\n",
    "        self.action_space = gym.spaces.Discrete(len(self.edge_space))\n",
    "        self.num_employees = self.initial_state[\"employee\"].x.shape[0]\n",
    "        self.num_shifts = self.initial_state[\"shift\"].x.shape[0]\n",
    "\n",
    "    def _get_num_consecutive_violations(self) -> int:\n",
    "        planning = self.get_current_planning()\n",
    "        num_consecutive_violations = 0\n",
    "        for i in range(self.num_shifts-1): \n",
    "            num_consecutive_violations = num_consecutive_violations + (Counter(planning[i])-(Counter(planning[i])-Counter(planning[i+1]))).total()  \n",
    "        return num_consecutive_violations\n",
    "\n",
    "    def idx2edge(self, idx: int) -> Tuple[int,int]:\n",
    "        return self.edge_space[idx]\n",
    "\n",
    "    def edge2idx(self, edge: Tuple[int,int]) -> int:\n",
    "        return self.edge_space.index(edge)\n",
    "\n",
    "    def get_num_planned_shifts(self) -> int:\n",
    "        return self.state[\"assigned\"].edge_index.shape[1]\n",
    "            \n",
    "    def get_current_planning(self) -> dict:\n",
    "        planning = dict() \n",
    "        for i in range(self.num_shifts):\n",
    "            planning[i] = list()\n",
    "        for i in torch.arange(self.state[\"assigned\"].edge_index.shape[1]): \n",
    "            planning[self.state[\"assigned\"].edge_index[:, i][1].item()].append(self.state[\"assigned\"].edge_index[:, i][0].item())\n",
    "        return planning\n",
    "\n",
    "    def info(self) -> dict:\n",
    "        return {}\n",
    "        \n",
    "    def step(self, action: int) -> tuple[HeteroData, float, bool, bool, dict]:\n",
    "        if not self.action_space.contains(action): \n",
    "            return (self.state, self.reward(), self.terminated(), self.truncated(), self.info())\n",
    "\n",
    "        action_edge = torch.tensor(self.idx2edge(action))\n",
    "        mask = torch.ones(self.state[\"assigned\"].edge_index.shape[1], dtype=torch.bool)\n",
    "        # check if shift exists \n",
    "        for i in torch.arange(self.state[\"assigned\"].edge_index.shape[1]): \n",
    "            if torch.equal(self.state[\"assigned\"].edge_index[:,i], action_edge):\n",
    "                mask[i] = False\n",
    "        # remove # if removing is allowed\n",
    "        #self.state[\"assigned\"].edge_index = self.state[\"assigned\"].edge_index[:,mask] \n",
    "        \n",
    "        # if no shift exists, create one\n",
    "        if not (~mask).any():\n",
    "            self.state[\"assigned\"].edge_index = torch.hstack((self.state[\"assigned\"].edge_index, action_edge[:,None]))\n",
    "            \n",
    "        return (copy.deepcopy(self.state), self.reward(), self.terminated(), self.truncated(), self.info())\n",
    "\n",
    "    def reset(self, seed: int = None) -> tuple[HeteroData, dict]:\n",
    "        super().reset(seed=seed)\n",
    "        self.state = self.initial_state\n",
    "        return (self.state, self.info())\n",
    "\n",
    "    def reward(self) -> float:\n",
    "        reward = 0\n",
    "        reward = reward + self.get_num_planned_shifts()\n",
    "        reward = reward - 10*self._get_num_consecutive_violations()\n",
    "        if self.terminated():\n",
    "            reward = reward + 1000\n",
    "        return reward\n",
    "\n",
    "    def terminated(self) -> bool:\n",
    "        terminated = True\n",
    "        planning = self.get_current_planning()\n",
    "        for i in range(self.num_shifts-1): \n",
    "            if len(planning[i])<2:\n",
    "                terminated = False\n",
    "                break\n",
    "        return terminated\n",
    "\n",
    "    def truncated(self) -> bool:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c34219-71de-4ac1-82f0-05b03faa816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "from torch import distributions\n",
    "from torch_geometric.nn import RGCNConv \n",
    "\n",
    "class RL_agent():\n",
    "    def __init__(self, gnn_employees, gnn_shifts): \n",
    "        self.gnn_employees = gnn_employees\n",
    "        self.gnn_shifts = gnn_shifts\n",
    "\n",
    "    def decode(self, emb_employees, emb_shifts):\n",
    "        num_employees = emb_employees.shape[0]\n",
    "        num_shifts = emb_shifts.shape[0]\n",
    "\n",
    "        expanded_shifts = emb_shifts.repeat((num_employees,1))\n",
    "        expanded_employees = emb_employees.repeat_interleave(num_shifts, dim=0)\n",
    "        dot_products = expanded_shifts.mul(expanded_employees).squeeze().sum(dim=1)\n",
    "\n",
    "        #employees_idx = torch.arange(num_employees).repeat_interleave(num_shifts, dim=0)\n",
    "        #shifts_idx = torch.arange(num_shifts).repeat(num_employees)\n",
    "        #edge_list = torch.stack((employees_idx, shifts_idx), dim=1)  \n",
    "        \n",
    "        return dot_products\n",
    "\n",
    "    def encode(self, state):\n",
    "        emb_shifts = gnn_employees(x=(state.x_dict[\"employee\"], state.x_dict[\"shift\"]), edge_index=state[\"assigned\"][\"edge_index\"], edge_type=torch.zeros(state[\"assigned\"][\"edge_index\"].shape[1], dtype=torch.int64))\n",
    "\n",
    "        assignments_flipped=torch.vstack((state[\"assigned\"][\"edge_index\"][1], state[\"assigned\"][\"edge_index\"][0])) \n",
    "        emb_employees = gnn_shifts(x=(state.x_dict[\"shift\"], state.x_dict[\"employee\"]), edge_index=assignments_flipped, edge_type=torch.zeros(state[\"assigned\"][\"edge_index\"].shape[1], dtype=torch.int64))\n",
    "\n",
    "        return emb_employees, emb_shifts\n",
    "        \n",
    "    def get_policy(self, state):\n",
    "        logits = self.decode(*self.encode(state)) \n",
    "        policy_distribution = distributions.categorical.Categorical(logits=logits)\n",
    "        return policy_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed147ed-b916-4dff-ba06-33f6485e1914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport networkx as nx \\nfrom torch_geometric.utils import to_networkx\\n\\ng=to_networkx(env.state)\\nnx.draw(g)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import networkx as nx \n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "g=to_networkx(env.state)\n",
    "nx.draw(g)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be7286-f3a3-4769-bc90-c54f6044849e",
   "metadata": {},
   "source": [
    "## Training environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5e43eb-e22d-4b3f-ab54-7f73e24bb54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "def get_random_employees(max_n=5):\n",
    "    n = random.randint(1,max_n)\n",
    "    return torch.tensor([[1]], dtype=torch.float).expand((n,1)) \n",
    "def get_week_shifts():\n",
    "    shifts = list()\n",
    "    shifts.append(torch.tensor([0,0,0,0,0,0,0])) # Monday, day\n",
    "    shifts.append(torch.tensor([0,0,0,0,0,0,1])) # Monday, night\n",
    "    shifts.append(torch.tensor([1,0,0,0,0,0,0])) # Tuesday, day\n",
    "    shifts.append(torch.tensor([1,0,0,0,0,0,1])) # Tuesday, night\n",
    "    shifts.append(torch.tensor([0,1,0,0,0,0,0])) # Wednesday, day\n",
    "    shifts.append(torch.tensor([0,1,0,0,0,0,1])) # Wednesday, night\n",
    "    shifts.append(torch.tensor([0,0,1,0,0,0,0])) # Thursday, day\n",
    "    shifts.append(torch.tensor([0,0,1,0,0,0,1])) # Thursday, night\n",
    "    shifts.append(torch.tensor([0,0,0,1,0,0,0])) # Friday, day\n",
    "    shifts.append(torch.tensor([0,0,0,1,0,0,1])) # Friday, night\n",
    "    shifts.append(torch.tensor([0,0,0,0,1,0,0])) # Saturday, day\n",
    "    shifts.append(torch.tensor([0,0,0,0,1,0,1])) # Saturday, night\n",
    "    shifts.append(torch.tensor([0,0,0,0,0,1,0])) # Sunday, day\n",
    "    shifts.append(torch.tensor([0,0,0,0,0,1,1])) # Sunday, night\n",
    "    shifts = torch.vstack(shifts)\n",
    "    shifts = shifts.to(torch.float)\n",
    "    return shifts\n",
    "def get_empty_assignments():\n",
    "    return torch.tensor([[],[]], dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ce41682-b911-45dd-b4bc-67b07a016eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, agent, max_steps = 20, env = None):\n",
    "        self.agent = agent\n",
    "        self.max_steps = max_steps\n",
    "        if env != None:\n",
    "            self.env = env \n",
    "        else:\n",
    "            self.env = PersonnelScheduleEnv(employees=get_random_employees(), shifts=get_week_shifts(), assignments=get_empty_assignments()) \n",
    "\n",
    "    def sample_episode(self):\n",
    "        state, _ = self.env.reset()\n",
    "        terminated = False\n",
    "        i=0\n",
    "        while not terminated and i < self.max_steps: \n",
    "            _, state, _, _ = self._step(state)\n",
    "            i = i + 1\n",
    "\n",
    "    def _step(self, current_state):  \n",
    "        action = self.agent.get_policy(current_state).sample().item()\n",
    "        \n",
    "        state, reward, terminated, _, _ = self.env.step(action)\n",
    "        return action, state, reward, terminated\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab935205-862f-4d71-a1cf-34fd6542b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# Experience Replay for RL\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"reward\", \"future_returns\"))\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.memory=deque([], maxlen=capacity)\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ae16d5d-49bd-4bbd-9734-953120ec67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class MemoryWrapper():\n",
    "    def __init__(self, actor, memory):\n",
    "        self.actor = actor\n",
    "        self.memory = memory\n",
    "\n",
    "    def sample_episode(self):\n",
    "        state, _ = self.actor.env.reset()\n",
    "        terminated = False\n",
    "        steps = []\n",
    "        i=0\n",
    "        while not terminated and i < self.actor.max_steps:\n",
    "            action, new_state, reward, terminated = self.actor._step(state)\n",
    "            steps.append((state, action, reward))\n",
    "            state = new_state\n",
    "            i = i + 1\n",
    "        transitions = []\n",
    "        future_returns=0\n",
    "        for step in reversed(steps):\n",
    "            future_returns = future_returns + step[2]\n",
    "            transitions.append(Transition(*step, future_returns)) \n",
    "        \n",
    "        for transition in transitions: \n",
    "            self.memory.push(transition) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2814d680-39cb-4322-a8b7-13d19e2b1147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch import distributions\n",
    "\n",
    "class Training():\n",
    "    def __init__(self, gnn_employees, gnn_shifts, tensorboard, device, max_steps=20, num_epoch=100, batch_size=128):\n",
    "        self.gnn_employees = gnn_employees\n",
    "        self.gnn_shifts = gnn_shifts \n",
    "        self.tensorboard = tensorboard\n",
    "        self.device = device\n",
    "        self.num_epoch = num_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = ReplayMemory(10000)\n",
    "        env = PersonnelScheduleEnv(employees=get_random_employees(), shifts=get_week_shifts(), assignments=get_empty_assignments(), device=device)\n",
    "        self.agent = RL_agent(gnn_employees, gnn_shifts) \n",
    "        self.actor = MemoryWrapper(Actor(self.agent, max_steps=max_steps, env=env), self.memory)\n",
    "    \n",
    "    def _gradient_update(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            #print(\"len(self.memory): \", len(self.memory))\n",
    "            return \n",
    "\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        #converts batch_array of Transitions to Transition of batch_arrays\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        state_batch = batch.state\n",
    "        action_batch = torch.tensor(batch.action, dtype=torch.int).to(self.device)\n",
    "        reward_batch = torch.tensor(batch.reward, dtype=torch.float).to(self.device)\n",
    "        future_returns_batch = torch.tensor(batch.future_returns, dtype=torch.float).to(self.device)\n",
    "\n",
    "        logits_list = list()\n",
    "        for i in torch.arange(self.batch_size):\n",
    "            logits = self.agent.decode(*self.agent.encode(state_batch[i]))\n",
    "            logits_list.append(logits)\n",
    "        logits_batch = torch.vstack(logits_list).to(self.device)\n",
    "\n",
    "        policy_distribution = distributions.categorical.Categorical(logits=logits_batch) \n",
    "        log_probs = policy_distribution.log_prob(action_batch)\n",
    "\n",
    "        loss = -(log_probs * future_returns_batch).sum() / self.batch_size\n",
    "        loss.backward()\n",
    "\n",
    "        self.tensorboard.add_scalar(\"loss\", loss.detach().cpu(), self.epoch)\n",
    "        self.tensorboard.add_scalar(\"avg_reward\", reward_batch.mean().detach().cpu(), self.epoch)\n",
    "        self.tensorboard.add_scalar(\"avg_future_returns\", future_returns_batch.mean().detach().cpu(), self.epoch)\n",
    "        \n",
    "    def start_training(self):\n",
    "        for epoch in tqdm(range(self.num_epoch)):\n",
    "            self.epoch = epoch\n",
    "            self.actor.sample_episode() \n",
    "            self._gradient_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd5a1ff-2334-4015-af1e-fc20e3770734",
   "metadata": {},
   "source": [
    "## Test suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34866725-a019-416a-88c7-2146df8b8ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb_summary = SummaryWriter(\"./\", purge_step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03972eab-f5f3-4e86-b277-0635e330b13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation successful\n",
      "The graph has  16  nodes\n",
      "The graph has  0  edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|â–                                                                             | 31/10000 [00:21<1:52:41,  1.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m gnn_shifts \u001b[38;5;241m=\u001b[39m RGCNConv(in_channels \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m1\u001b[39m), out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_relations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m training \u001b[38;5;241m=\u001b[39m Training(gnn_employees, gnn_shifts, tb_summary, device, num_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 51\u001b[0m, in \u001b[0;36mTraining.start_training\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39msample_episode() \n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 33\u001b[0m, in \u001b[0;36mTraining._gradient_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m logits_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[0;32m---> 33\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     34\u001b[0m     logits_list\u001b[38;5;241m.\u001b[39mappend(logits)\n\u001b[1;32m     35\u001b[0m logits_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack(logits_list)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mRL_agent.encode\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     26\u001b[0m emb_shifts \u001b[38;5;241m=\u001b[39m gnn_employees(x\u001b[38;5;241m=\u001b[39m(state\u001b[38;5;241m.\u001b[39mx_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memployee\u001b[39m\u001b[38;5;124m\"\u001b[39m], state\u001b[38;5;241m.\u001b[39mx_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshift\u001b[39m\u001b[38;5;124m\"\u001b[39m]), edge_index\u001b[38;5;241m=\u001b[39mstate[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massigned\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m\"\u001b[39m], edge_type\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mzeros(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massigned\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64))\n\u001b[1;32m     28\u001b[0m assignments_flipped\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mvstack((state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massigned\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m], state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massigned\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])) \n\u001b[0;32m---> 29\u001b[0m emb_employees \u001b[38;5;241m=\u001b[39m \u001b[43mgnn_shifts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshift\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43memployee\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massignments_flipped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massigned\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43medge_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m emb_employees, emb_shifts\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gnn_employees = RGCNConv(in_channels = (1,7), out_channels=2, num_relations=1).to(device)\n",
    "gnn_shifts = RGCNConv(in_channels = (7,1), out_channels=2, num_relations=1).to(device)\n",
    "training = Training(gnn_employees, gnn_shifts, tb_summary, device, num_epoch=10000)\n",
    "training.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3a4c6d-0716-4e8f-96f4-ffe6a2717524",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(training.actor.actor.env.edge_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d40cd71-5c5a-40e2-8f1c-fd94eea69879",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1,test2=agent.encode(state)\n",
    "print(test1.shape)\n",
    "print(test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c03238-3f00-4053-acfd-ff33333fa46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e8c5ac-f936-4afc-be25-705c3b4c309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RL_agent(gnn_employees, gnn_shifts)\n",
    "env = PersonnelScheduleEnv(employees=get_random_employees(), shifts=get_week_shifts(), assignments=get_empty_assignments()) \n",
    "actor = Actor(agent, max_steps=20, env=env) \n",
    "\n",
    "state, _ = env.reset()\n",
    "print(\"state: \", state) \n",
    "print(\"encode state: \", agent.encode(state))\n",
    "\n",
    "scores = agent.decode_sequential(*agent.encode(state))\n",
    "print(\"scores sequential: \", scores)\n",
    "print(\"best score: \", heapq.heappop(scores)[1])\n",
    "scores, actions = agent.decode_matrix(*agent.encode(state))\n",
    "print(\"scores matrix: \", scores)\n",
    "print(\"correspondig actions: \", actions)\n",
    "print(\"best action: \", actions[scores.argmax()])\n",
    "\n",
    "action = agent.get_action(state)\n",
    "print(\"action: \", action)\n",
    "state, reward, terminated, _ , _ = env.step(action)\n",
    "print(\"state: \", state)\n",
    "print(\"reward: \", reward)\n",
    "print(\"terminated: \", terminated)\n",
    "\n",
    "scores = agent.decode_sequential(*agent.encode(state))\n",
    "print(\"scores: \", scores)\n",
    "\n",
    "action = agent.get_action(state)\n",
    "print(\"action: \", action)\n",
    "state, reward, terminated, _ , _ = env.step(action)\n",
    "print(\"state: \", state)\n",
    "print(\"reward: \", reward)\n",
    "print(\"terminated: \", terminated)\n",
    "\n",
    "print(actor.sample_episode())\n",
    "\n",
    "memory = ReplayMemory(100)\n",
    "actor = MemoryWrapper(actor, memory)\n",
    "actor.sample_episode()\n",
    "print(memory.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef0127-26f7-4177-9d57-854b71a1b2da",
   "metadata": {},
   "source": [
    "## Save as python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "211f4f10-9fd8-43c6-aff2-6a98a473affb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook RL_env.ipynb to script\n",
      "[NbConvertApp] Writing 14066 bytes to RL_env.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "!jupyter nbconvert --to script \"RL_env.ipynb\"\n",
    "filename = \"RL_env.py\"\n",
    "\n",
    "# delete this cell from python file\n",
    "lines = []\n",
    "with open(filename, 'r') as fp:\n",
    "    lines = fp.readlines()\n",
    "with open(filename, 'w') as fp:\n",
    "    for number, line in enumerate(lines):\n",
    "        if number < len(lines)-17: \n",
    "            fp.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
